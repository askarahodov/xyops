# Серверы

## Обзор

Серверы — это worker-узлы кластера xyOps. Каждый сервер запускает легковесный агент (xySat), поддерживает постоянное WebSocket соединение с conductor, собирает мониторинговые метрики и выполняет задачи по запросу. Сервер может быть физическим хостом, VM или контейнером и работать на Linux, macOS или Windows.

Этот документ объясняет роль серверов в xyOps, как их добавлять и организовывать, как события таргетируют сервера, что видно на странице сервера в UI и как система масштабируется до больших парков.

## Ключевые моменты

- Серверы запускают xySat и выступают job runners и сборщиками метрик.
- Conductors запускают полный стек xyOps и координируют расписание, маршрутизацию, storage и UI.
- В кластер можно добавить любое количество серверов и conductors; агенты поддерживают живые соединения и auto-failover между conductors.
- Серверы собирают "быстрые" метрики каждую секунду (CPU/Mem/Disk/Net) и минутные метрики через пользовательские monitor plugins. Некоторые метрики недоступны на Windows.

## Серверы и conductors

- **Server**: worker-узел с xySat. Он отправляет данные хоста и метрики, и выполняет задачи от conductor. Серверы можно группировать и таргетировать событиями.
- **Conductor**: полноценный инстанс xyOps (primary или hot standby), который управляет расписанием, маршрутизирует задачи, хранит данные и обслуживает UI/API. В кластере может быть несколько conductors для отказоустойчивости; в каждый момент времени один — primary.

xySat хранит актуальный список conductors. Если сервер теряет primary соединение, он автоматически переключается на backup и после выборов переподключается к новому primary.

## Добавление серверов

Есть три способа добавить сервер:

1. **Через UI** (one-line installer)
	- Откройте вкладку Servers и нажмите "Add Server:".
	- При желании задайте label, icon, enabled и выберите groups (или оставьте авто-группировку).
	- Скопируйте готовую однострочную команду установки для Docker, Linux, macOS или Windows и выполните ее на целевом хосте.
	- Инсталлятор аутентифицируется, устанавливает xySat как сервис автозапуска (systemd/launchd/Windows Service), пишет конфиг и запускает агент.
	- Сервер сразу появляется в кластере, начинает стримить метрики и готов выполнять задачи.
2. **Автоматический bootstrap** (API Key)
	- Для autoscaling или эфемерных хостов создайте API Key и используйте провижининг, чтобы вызвать bootstrap endpoint и получить server token и команду установки при первом запуске.
	- См. подробности ниже. Можно включить это в cloud-init, AMI, Packer templates или кастомные init скрипты.
3. **Ручная установка**
	- Установите xySat на хост и настройте его на URL кластера и secret key. Secret key используется для генерации auth token. Запустите сервис, чтобы подключиться к кластеру.
	- Обычно используется только для разработки, тестов и домашних лабораторий.

Примечания:

- Auth токены сервера не истекают. При необходимости можно [провернуть секретный ключ](hosting.md#secret-key-rotation) (это перегенерирует все токены) из UI.
- Обновления xySat оркестрируются из UI и спроектированы так, чтобы не прерывать выполняющиеся задачи.

### Автоматический bootstrap сервера

Для автоматизации добавления новых эфемерных серверов в кластер выполните следующие шаги:

Сначала создайте [API Key](api.md#api-keys) в UI и назначьте ему только привилегию [add_servers](privileges.md#add_servers) (удалите все дефолтные). Затем нажмите "Add Server" в UI и скопируйте команду установки для Linux. Не вводите параметры сервера вроде label, icon или group.

Замените auth token (он истекает через 24 часа) на ваш новый API Key (он не истекает). Токен — значение параметра `t` в query string URL. Пример:

```sh
curl -s "https://xyops01.mycompany.com/api/app/satellite/install?t=API_KEY_HERE" | sudo sh
```

Затем вставьте новую команду в ваш скрипт провижининга, именно в этапе first-boot.

Примечания:

- Убедитесь, что сетевой стек сервера поднят до запуска bootstrap команды.
- После первого скачивания xySat будет устанавливать из локального кэша и не будет выходить в интернет (либо используйте [Air-Gapped Mode](hosting.md#air-gapped-mode)).
- Убедитесь, что на серверах установлен `curl`. Иначе перепишите команду на `wget`.
- В автоматическом режиме hostname сервера определяет, в какие группы он попадет.

## Группы и авто-назначение

Серверы могут входить в одну или несколько групп. Группы используются для организации парка, ограничения мониторов/алертов и таргетинга событий.

- **Auto-assignment**: группы могут задать regex для hostname. Когда сервер появляется (или меняет hostname), совпавшие группы назначаются автоматически.
- **Multiple groups**: сервер может подходить под несколько групп.
- **Manual assignment**: если вы вручную назначили группы для сервера, авто-назначение по hostname отключается. Его можно вернуть, очистив список групп вручную.
- **Re-evaluation**: совпадения пересчитываются при изменении hostname сервера.

См. [Server Groups](groups.md) для подробностей.

## Таргетинг событий на серверы

События задают targets как список ID серверов и/или ID групп. Во время запуска планировщик разворачивает их в набор текущих онлайн и включенных серверов и выбирает один по алгоритму события (random, round_robin, least_cpu, least_mem или monitor-based). См. [Event.targets](data.md#event-targets) и [Event.algo](data.md#event-algo).

Поведение при офлайне серверов:

- **Single-server target**: если целевой сервер офлайн, поведение настраивается limits: добавьте Queue limit, чтобы поставить в очередь; без него задача падает сразу.
- **Group target**: офлайн сервера игнорируются; выбираются другие онлайн сервера из группы.

Алерты могут подавлять запуск задач на конкретном сервере, поэтому сервер с активным алертом может быть исключен из выбора до снятия алерта. Эта функция настраивается на уровне алерта (см. [Alerts](alerts.md)).

## UI сервера

У каждого сервера есть отдельная страница в UI, показывающая текущее и историческое состояние:

- **Status**: Online/offline, label/hostname, IP, OS/arch, детали CPU, память, виртуализация, версия агента, uptime и группы.
- **Quick metrics** (каждую секунду): маленькие rolling графики CPU, памяти, диска и сети за последние 60 секунд.
- **Monitors** (каждую минуту): графики всех пользовательских monitors и deltas, с наложением алертов.
- **Processes**: текущая таблица процессов с PID/parent/CPU/memory/network и другими метриками.
- **Connections**: текущие сетевые соединения со статусом, source/dest IP и метриками передачи.
- **Running jobs**: активные задачи, выполняющиеся на сервере, включая workflow parents/children.
- **Upcoming jobs**: прогнозируемые задачи, которые должны попасть на этот сервер (по targets и расписанию событий).
- **Alerts**: активные алерты, затрагивающие этот сервер, со ссылками на историю.
- **User Actions**: сделать snapshot, поставить watch, редактировать данные сервера (label, enabled/disabled, icon, groups) или удалить сервер.

Search по парку и истории доступен в Servers -> Search. Можно фильтровать по группе, OS platform/distro/release/arch, CPU brand/cores и диапазонам created/modified.

## Snapshots и Watches

Snapshots сохраняют текущее состояние сервера для последующего анализа и сравнения. Они доступны в Snapshots и через действия/алерты.

Что содержит snapshot:

- Полный список процессов (аналог `ps -ef`), сетевые соединения (включая listeners), disk mounts, network devices.
- Факты хоста: тип CPU, количество ядер, max RAM, OS platform/distro/release, uptime, load и т.д.
- Последние 60 секунд "быстрых" метрик (CPU/Mem/Disk/Net).
- Ссылки на активные задачи и релевантные алерты на момент снимка.

Как создаются snapshots:

- Вручную: "Create Snapshot" на странице сервера.
- Actions: добавить Snapshot action в задачу или алерт.
- Watch: запустить watch на сервере для снимка каждую минуту заданное время (по умолчанию 5 минут).

Хранение:

- Snapshots хранятся до глобального лимита (по умолчанию 100,000) и чистятся ночью.

См. [Snapshots](snapshots.md) для деталей.

## Метрики и частота

- Каждую секунду (quick): CPU, memory, disk и network; последние 60 секунд хранятся в памяти для UI.
- Каждую минуту (monitors): пользовательские monitor plugins запускаются каждую минуту и возвращают числовые значения (или deltas). Эти данные используются для графиков, алертов и дашбордов. См. [Monitors](monitors.md).
- Отличия ОС: некоторые метрики недоступны на Windows.

Чтобы избежать эффекта thundering herd на conductors, каждый сервер детерминированно смещает минутный сбор данных, используя хэш Server ID и динамически вычисляемый offset. Это равномерно распределяет отправку по N секундам, в зависимости от размера кластера. Быстрые секундные метрики делают то же, но смещают на миллисекунды.

## Жизненный цикл и здоровье

- **Online/offline**: сервер онлайн, пока его WebSocket соединение xySat активно. Если сокет падает, сервер сразу отмечается offline. UI обновляется в реальном времени.
- **Running jobs**: задачи не прерываются сразу при offline сервера. Conductors ждут `dead_job_timeout`, прежде чем считать задачу мертвой и прервать ее (по умолчанию 120 секунд). См. [Configuration](config.md#dead_job_timeout).
- **Enable/disable**: отключение сервера убирает его из выбора задач, но он может оставаться online и продолжать отправлять метрики.

## Масштабируемость

xyOps рассчитан на большие парки и тестировался до сотен серверов в кластере. Для крупных кластеров:

- Детерминированное распределение не дает всем серверам отправлять минутные/секундные метрики одновременно; нагрузка равномерно распределяется.
- Conductors должны работать на мощном железе (CPU/RAM/SSD) для стабильной обработки данных, выборов и обслуживания UI/API.
- Можно использовать несколько conductors (primary + hot standby). Агенты авто-failover'ят между ними; кластер проводит выборы нового primary по необходимости.

См. также [Scaling](scaling.md).

## Вывод серверов из эксплуатации

Чтобы вывести сервер из эксплуатации, откройте его страницу и нажмите иконку корзины:

- **Online**: conductor отправит агенту команду uninstall, которая остановит и удалит xySat. Можно дополнительно удалить исторические данные (запись сервера, метрики, snapshots).
- **Offline**: сервер можно удалить, но нужно явно удалить историю, так как uninstall требует активного соединения.

Удаление необратимо.

## Связанные данные и API

- Data: [Server](data.md#server), [ServerMonitorData](data.md#servermonitordata), [Snapshot](data.md#snapshot), [Group](data.md#group).
- Servers API: [get_active_servers](api.md#get_active_servers), [get_active_server](api.md#get_active_server), [get_server](api.md#get_server), [update_server](api.md#update_server), [delete_server](api.md#delete_server), [watch_server](api.md#watch_server), [create_snapshot](api.md#create_snapshot).
- Search: [search_servers](api.md#search_servers), server summaries и поиск по snapshots.
